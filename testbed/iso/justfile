export SSHPASS := "giraff"
export SSH_CMD := "sshpass -e ssh -t -oUserKnownHostsFile=/dev/null -oStrictHostKeyChecking=no root@127.0.0.1 -q -p"
export SCP_CMD := "sshpass -e scp -oUserKnownHostsFile=/dev/null -oStrictHostKeyChecking=no -P"

# Entrypoint, run deploy and then tunnels
_default:
    @just --list

fmt:
    nix --experimental-features "nix-command flakes" fmt .

build-vm:
    #!/usr/bin/env bash
    set -e
    rm -f ./sda.raw || true
    script_vm=$(nix build --extra-experimental-features nix-command --extra-experimental-features flakes .#nixosConfigurations.node_vm.config.system.build.diskoImagesScript --print-out-paths --no-link --quiet)
    $script_vm --build-memory 4096


build-vagrant:
    #!/usr/bin/env bash
    set -e
    rm -f ./sda.raw || true
    script_vm=$(nix build --extra-experimental-features nix-command --extra-experimental-features flakes .#nixosConfigurations.vagrant.config.system.build.diskoImagesScript --print-out-paths --no-link --quiet)
    $script_vm --build-memory 6096

    mkdir -p .vagrant
    qemu-img convert -c -f raw -O qcow2 sda.raw .vagrant/sda.qcow2
    mv .vagrant/sda.qcow2 .vagrant/box.img
    cat <<EOF > .vagrant/metadata.json
    {
       "provider"     : "libvirt",
       "format"       : "qcow2",
       "virtual_size" : 40
    }
    EOF
    cat <<EOF > .vagrant/Vagrantfile
    Vagrant.configure("2") do |config|
             config.vm.provider :libvirt do |libvirt|
             libvirt.driver = "kvm"
             libvirt.host = 'localhost'
             libvirt.uri = 'qemu:///system'
             end
    config.vm.define "new" do |giraffbox|
             giraffbox.vm.box = "giraffbox"
             giraffbox.vm.provider :libvirt do |test|
             test.memory = 1024
             test.cpus = 1
             end
             end
    end
    EOF
    echo "Installing directly the box into ~/.vagrant.d/boxes/giraffbox/0/libvirt/"
    mkdir -p ~/.vagrant.d/boxes/giraffbox/0/libvirt/
    cp -r .vagrant/* ~/.vagrant.d/boxes/giraffbox/0/libvirt/


update:
    nix flake update --experimental-features "nix-command flakes"

ssh-in:
    @{{ SSH_CMD }} 2221

ssh-in-build:
    @{{ SSH_CMD }} 2223

upload city="rennes": build-vm
    #!/usr/bin/env bash
    set -e
    vm_path=./sda.raw

    qemu-kvm \
        -cpu max \
        -name giraff.fog_node_vm \
        -m 20000 \
        -smp 16 \
        -drive cache=writeback,file="$vm_path",id=giraff.drive.fog_node_vm,if=none,index=1,werror=report -device virtio-blk-pci,drive=giraff.drive.fog_node_vm \
        -net nic,netdev=giraff.fog_node_vm,model=virtio -netdev user,id=giraff.fog_node_vm,hostfwd=tcp::2223-:22,hostfwd=tcp::5555-:5555 \
        -enable-kvm \
        -nographic > /dev/null&
    VM_PID=$!

    until $SSH_CMD 2223 "k3s kubectl get pods -A" 2> /dev/null
    do
        sleep 5
    done

    until $SSH_CMD 2223 "k3s kubectl wait pods -n openfaas -l app=gateway --for condition=Ready --timeout=120s" 2> /dev/null
    do
        sleep 5
    done

    until $SSH_CMD 2223 "k3s kubectl wait pods -n openfaas -l app=prometheus --for condition=Ready --timeout=120s" 2> /dev/null
    do
        sleep 5
    done

    just cache_functions

    $SSH_CMD 2223 "poweroff -ff" || true

    wait $VM_PID || true

    temp=./sda.qcow2
    qemu-img convert -c -f raw -O qcow2 $vm_path $temp

    rsync -chavzq --inplace --stats --perms --chmod=u+rwx,g+rwx,o+rwx "$temp" {{ city }}.grid5000.fr:~/nixos.qcow2


cache_functions:
    #!/usr/bin/env bash
    export IMAGE_REGISTRY=localhost:5555
    export FUNCTION_DESCRIPTIONS=$(find "{{justfile_directory() / '..' / 'pipelines'}}" -name *.jsonc)
    nix develop --extra-experimental-features 'nix-command flakes' .#testbed -c just _cache_functions

_cache_functions:
    #!/usr/bin/env python
    import sys
    sys.path.append('../')
    from function import load_function_descriptions, build_functions_to_registry
    import asyncio
    ips = ["localhost"]
    function_descriptions = load_function_descriptions()
    asyncio.run(build_functions_to_registry(function_descriptions, ips))

_run_vm disk="sda.raw":
    #!/usr/bin/env bash
    set -e
    cp {{ disk }} nixos.qcow2
    temp=nixos.qcow2
    chmod u+rwx $temp

        #-drive cache=writeback,throttling.bps-write=2000000,throttling.iops-total=6000,file="$temp",id=giraff.drive.dev,if=none,index=1,werror=report -device virtio-blk-pci,drive=giraff.drive.dev \
    qemu-kvm \
        -cpu max \
        -name giraff.dev \
        -m 8000 \
        -smp 4 \
        -drive cache=writeback,file="$temp",id=giraff.drive.dev,if=none,index=1,werror=report -device virtio-blk-pci,drive=giraff.drive.dev \
        -net nic,netdev=giraff.dev,model=virtio \
        -netdev user,id=giraff.dev,hostfwd=tcp::2221-:22,hostfwd=tcp::16686-:16686,hostfwd=tcp::14268-:14268,hostfwd=tcp::4317-:4317,hostfwd=tcp::8080-:8080,hostfwd=tcp::31112-:31112,hostfwd=tcp::9086-:9086,hostfwd=tcp::3128-:3128,hostfwd=tcp::6443-:6443,hostfwd=tcp::5555-:5555\
        -enable-kvm \
        -nographic&
    wait

_openfaas_in_vm:
    #!/usr/bin/env bash
    set -e

    (rm vm.kubeconfig || true) 2> /dev/null
    while ! timeout 5 $SSH_CMD 2221 echo OK > /dev/null 2>&1; do sleep 2; done
    $SSH_CMD 2221 "cat /etc/rancher/k3s/k3s.yaml>&1" > vm.kubeconfig 2>&2
    echo -e "[kubeconfig] copied \033[32mOK\033[0m"

    retry_failed () {
      while ! sh -c "$1"; do
        echo "redoing $1"
        sleep 1
      done
    }

    open_jaeger () {
      set +e
      $SSH_CMD 2221 'docker start jaeger' >/dev/null 2>&1 || $SSH_CMD 2221 'docker run -d --rm --name jaeger -e COLLECTOR_OTLP_ENABLED=true -p 5775:5775/udp -p 6831:6831/udp -p 6832:6832/udp -p 5778:5778 -p 16686:16686 -p 14268:14268 -p 4317:4317 -p 4318:4318 jaegertracing/all-in-one:1.51.0' >/dev/null 2>&1
      status=$?
      if [ $status -eq 0 ]; then
        echo -e "[jaeger] jaeger \033[32mOK\033[0m"
      else
        echo -e "[jaeger] jaeger \033[31mFAILED\033[0m"
      fi
      exit $status

      #retry_failed "$SSH_CMD 2221 -N -g -L 14268:127.0.0.1:14268"&
      #retry_failed "$SSH_CMD 2221 -N -g -L 16686:127.0.0.1:16686"&
      #retry_failed "$SSH_CMD 2221 -N -g -L 4317:127.0.0.1:4317"&
      #retry_failed "$SSH_CMD 2221 -N -g -L 4318:127.0.0.1:4318"&
      #retry_failed "$SSH_CMD 2221 -N -g -L 5775:127.0.0.1:5775"&
      #retry_failed "$SSH_CMD 2221 -N -g -L 6831:127.0.0.1:6831"&
      #retry_failed "$SSH_CMD 2221 -N -g -L 6832:127.0.0.1:6832"&
      #retry_failed "$SSH_CMD 2221 -N -g -L 5778:127.0.0.1:5778"&
    }

    open_openfaas () {
      until $SSH_CMD 2221 "k3s kubectl get pods -A > /dev/null 2>&1" ; do sleep 2; done

      until $SSH_CMD 2221 "k3s kubectl wait pods -n openfaas -l app=gateway --for condition=Ready --timeout=120s > /dev/null 2>&1"; do sleep 2; done
      until $SSH_CMD 2221 "k3s kubectl wait pods -n openfaas -l app=prometheus --for condition=Ready --timeout=120s > /dev/null 2>&1"; do sleep 2; done

      (curl localhost:8080 >/dev/null 2>&1 || retry_failed "$SSH_CMD 2221 'k3s kubectl port-forward -n openfaas svc/gateway 8080:8080'")&
      echo -e "[openfaas] openfaas \033[32mOK\033[0m"

      wait

      #retry_failed "$SSH_CMD 2221 curl localhost:8080l
      #retry_failed "$SSH_CMD 2221 -N -g -L 8080:127.0.0.1:8080"&
    }

    open_influx () {
      retry_failed "$SSH_CMD 2221 sh -c 'systemctl is-active --quiet influxdb2 > /dev/null'"
      echo -e "[influxdb2] influxdb2 \033[32mOK\033[0m"
    #  retry_failed "$SSH_CMD 2221 -N -g -L 9090:127.0.0.1:9090"&
    #  retry_failed "$SSH_CMD 2221 -N -g -L 9086:127.0.0.1:9086"&
    #  retry_failed "$SSH_CMD 2221 -N -g -L 6443:127.0.0.1:6443"&
    }

    open_proxy () {
      retry_failed "$SSH_CMD 2221 curl localhost:3128" >/dev/null 2>&1
      #retry_failed "$SSH_CMD 2221 -N -g -L 3128:127.0.0.1:3128"&
      echo -e "[proxy] proxy \033[32mOK\033[0m"
    }

    open_jaeger&
    open_openfaas&
    open_influx&
    open_proxy&

    while true; do sleep 100; done

    wait

vm: build-vm
    mprocs "just _run_vm" "just _openfaas_in_vm"

vm-nocopy:
    mprocs "just _run_vm" "just _openfaas_in_vm"
